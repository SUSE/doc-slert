<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
                 type="text/xml" 
                 title="Profiling step"?>
<!DOCTYPE article
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>

<article version="5.0" xml:id="art.slert.virtguide" xml:lang="en"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
  <?suse-quickstart color="suse"?>
  <title>Virtualization Guide</title>
  <subtitle>&productname; &productnumber;</subtitle>
  <info>
   <productname>&productname;</productname>
   <productnumber>&productnumber;</productnumber>
   <date><?dbtimestamp?></date>
   <abstract>
      <para>It is possible to achieve isolation of real-time
        workloads running alongside KVM by using standard methods. For example,
        cpusets and routing IRQs to dedicated CPUs, all of which can be
        achieved by using the <command>cset</command> utility. Both
        libvirtd and KVM work fine in such configurations.</para>
      <para>System configurations that share CPUs between both RT and
        KVM workloads are not supported; proper isolation of workloads
        is imperative for achieving RT deadline constraints. </para>
      <para>None of the below observations and recommendations are
        specific to virtualization. Nevertheless, they can be considered
          <quote>best-effort</quote> for isolating RT and KVM
        workloads.</para>
    </abstract>
  </info>
  
  <sect1 xml:id="sec.virtguide.setup">
    <title>Setup</title>
    <para>All examples were carried out on a 48-core Xeon machine with 2
      NUMA nodes and 64GB of RAM running &slea;12 RT and the 3.12.49-rt
      kernel. The virtual machine was installed with <command>vm-install</command>, running
      SLE12 SP2 on 4 CPUs and 2GB of memory. The disk used was physical
      disk <filename>/dev/sdb</filename> as recommended by the &suse;
      virtualization documentation.</para>
    <para>The <command>cpuset</command> utility was used to shield the
      RT workload from KVM as described in the
      <citetitle>&slea; RT Shielding Guide</citetitle> (see
      <xref linkend="book.slert.shielding"/>):
    </para>
    <screen>cset shield --kthread=on -c 8-47</screen>
    <para>Affinity for the KVM vCPU tasks was modified via the <command>virsh
     vcpupin</command> command, with a 1-1 mapping. For example, vCPU&nbsp;0
     pinned to CPU&nbsp;0, etc.</para>
    <para>The CPUs were split into two groups. CPU 0-7 were allocated to
      the <systemitem>system</systemitem> cpuset and CPU 8-47 were allocated
      to the <systemitem>user</systemitem> group. Having CPUs on the same
      socket in two groups was done intentionally to monitor the effects on
      shared CPU resources, such as LLC.</para>
    <para>The RT workload used throughout is cyclictest, executed like so:</para>
    <screen>cset shield --exec cyclictest -- -a 8-47 -t 40 -n -m -p99 -d 0 -D 120 --quiet</screen>
  </sect1>
  
  <sect1 xml:id="sec.virtguide.observations">
    <title>Observations</title>
    <para>The following observations were made:</para>
    <orderedlist>
      <listitem>
        <para>VM Heavy I/O</para>
        <para>The test for this was to do the following in a VM:</para>
        <screen>dd if=/dev/zero of=empty bs=4096 count=$(((80*1024*1024)/4096))</screen>
        <para>Doing large amounts of disk I/O in the VM guests has a
          noticeable impact on the latency of RT tasks. This is because of
          the constant eviction of LLC data, resulting in more cache
          misses.</para>
        <para>The maximum latencies in for the real-time workload are seen on
          those CPUs on the same socket as the CPUs available to the KVM
          workload. For example, where the LLC is a shared resource between the
            <systemitem>system</systemitem> and
            <systemitem>user</systemitem> cpuset.</para>
      </listitem>
      <listitem>
        <para>cpufreq drivers incur timer latency</para>
        <para>Drivers like <systemitem>intel_pstate</systemitem> will set up a
          timer on each CPU to periodically sample and adjust the CPU's
          current P-state.
          If this fires at an inopportune time it can add delays to the
          scheduling of RT tasks, particularly because lots of the
          IRQ/timer code paths run with interrupts disabled.</para>
      </listitem>
      <listitem>
        <para>Interrupt handling introduces delays</para>
        <para>The handling of interrupts can result in latencies that
          affect RT workloads. Interrupts should be routed to
          <quote>housekeeping</quote> CPUs that are not running RT
          applications.</para>
      </listitem>
      <listitem>
        <para>Some kernel threads cannot be controlled with cpuset</para>
        <para>Performing heavy I/O in the VM may cause kthreads to be
          scheduled on the CPUs dedicated for RT. This can occur, for
          example, when a kthread is flushing dirty pages to disk.
        </para>
        <para>While it is impossible to move some kworker threads into
          the <systemitem>system</systemitem> cpuset, the above issue can be
          mitigated by setting the CPU affinity for those threads via:</para>
        <screen>/sys/devices/virtual/workqueue/writeback/cpumask</screen>
      </listitem>
    </orderedlist>
  </sect1>
  
  <sect1 xml:id="sec.virtguide.rec">
    <title>Recommendations</title>
    <para>Suggestions for tuning machines running both RT and KVM
      workloads are as follows:</para>
    <orderedlist>
      <listitem>
        <para>Affinitize RT tasks to their own CPUs, and if possible, to
          CPUs on their own dedicated socket. Using a dedicated socket
          avoids the issue from observation in <xref
           linkend="sec.virtguide.observations"/> above where the LLC
          occupancy is churned by VMs doing lots of I/O operations. If
          that is not an option some customers should look at
          Intel's Cache Allocation Technology to further enforce cache
          allocation policies.</para>
      </listitem>
      <listitem>
        <para>Disable drivers that arm per-cpu timers such as cpufreq
          drivers, for example, <code>intel_pstate=disable</code>.</para>
      </listitem>
      <listitem>
        <para>Set IRQ affinity to CPUs that are not running RT workloads
          and disable irqbalance.</para>
      </listitem>
      <listitem>
        <para>Set IRQ affinity to CPUs that are not running RT workloads.
          This can be achieved by setting the <envar>IRQBALANCE_BANNED_CPUS</envar>
          environment variable used by <command>irqbalance</command>(1) with
          a bitmask of banned CPUs. For the examples used throughout this
          document the following setting was used:</para>
        <screen>IRQBALANCE_BANNED_CPUS="ffff,ffffff00"</screen>
      </listitem>
      <listitem>
        <para>Search for cpumask control files in 
          <filename>/sys</filename> and set them
          appropriately for those cases that cannot be controlled via
          cpuset. The following command will list those files:</para>
        <screen>find /sys -name cpumask</screen>
      </listitem>
    </orderedlist>
  </sect1>
</article>
