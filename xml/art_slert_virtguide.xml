<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook51-profile.xsl"
                 type="text/xml" 
                 title="Profiling step"?>
<!DOCTYPE article
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>

<article version="5.0" xml:id="art.slert.virtguide" xml:lang="en"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
  <?suse-quickstart color="suse"?>
  <title>Virtualization Guide</title>
  <subtitle>&productname; &productnumber;</subtitle>
  <info>
    <abstract>
      <para>It is possible to achieve isolation of real time (RT)
        workloads running alongside KVM by using standard methods, like
        cpusets and routing IRQs to dedicated CPUs, all of which can be
        achieved by using the <command>cset</command> utility. Both
        libvirtd and KVM work fine in such configurations.</para>
      <para>System configurations that share CPUs between both RT and
        KVM workloads are not supported; proper isolation of workloads
        is imperative for achieving RT deadline constraints. </para>
      <para>None of the below observations and recommendations are
        specific to virtualization. Nevertheless, they can be considered
          <quote>best-effort</quote> for isolating RT and KVM
        workloads.</para>
    </abstract>
  </info>
  
  <sect1 xml:id="sec.virtguide.setup">
    <title>Setup</title>
    <para>All examples were carried out on a 48-core Xeon machine with 2
      NUMA nodes and 64GB of RAM running &slea;12 RT and the 3.12.49-rt
      kernel. The virtual machine was installed with <command>vm-install</command>, running
      SLE12 SP1 on 4 CPUs and 2GB of memory. The disk used was physical
      disk <filename>/dev/sdb</filename> as recommended by the &suse;
      virtualization documentation.</para>
    <para>The <command>cpuset</command> utility was used to shield the
      RT workload from KVM as described in the
      <citetitle>&slea; RT Shielding Guide</citetitle> (see
      <xref linkend="book.slert.shielding"/>):
    </para>
    <screen>cset shield --kthread=on -c 8-47</screen>
    <para>Affinity for the KVM vCPU tasks was modified via the <command>virsh</command>
      <command>vcpupin</command> command, with a 1-1 mapping, for example,
      vCPU 0 pinned to CPU 0, etc.</para>
    <para>The CPUs were split into two groups. CPU 0-7 were allocated to
      the <systemitem>system</systemitem> cpuset and CPU 8-47 were allocated
      to the <systemitem>user</systemitem> group. Having CPUs on the same
      socket in two groups was done intentionally to monitor the effects on
      shared CPU resources, such as LLC.</para>
    <para>The RT workload used throughout is cyclictest, executed like so:</para>
    <screen>cset shield --exec cyclictest -- -a 8-47 -t 40 -n -m -p99 -d 0 -D 120 --quiet</screen>
  </sect1>
  
  <sect1 xml:id="sec.virtguide.observations">
    <title>Observations</title>
    <orderedlist>
      <listitem xml:id="li.virtguide.observ.vmheavyio">
        <para>VM Heavy I/O</para>
        <para>The test for this was to do the following in a VM:</para>
        <screen>dd if=/dev/zero of=empty bs=4096 count=$(((80*1024*1024)/4096))</screen>
        <para>Doing large amounts of disk I/O in the VM guests has a
          noticeable impact on the latency of RT tasks. This is due to
          the constant eviction of LLC data, resulting in more cache
          misses.</para>
        <para>The maximum latencies in for the RT workload are seen on
          those CPUs on the same socket as the CPUs available to the KVM
          workload, for example, where the LLC is a shared resource between the
            <systemitem>system</systemitem> and
            <systemitem>user</systemitem> cpuset.</para>
      </listitem>
      <listitem>
        <para>cpufreq drivers incur timer latency</para>
        <para>Drivers like <systemitem>intel_pstate</systemitem> will setup a
          timer on each CPU to periodically sample and adjust the CPU's
          current P-state.
          If this fires at an inopportune time it can add delays to the
          scheduling of RT tasks, particularly because lots of the
          IRQ/timer code paths run with interrupts disabled.</para>
      </listitem>
      <listitem>
        <para>Interrupt handling introduces delays</para>
        <para>The handling of interrupts can result in latencies that
          affect RT workloads. Interrupts should be routed to
          <quote>housekeeping</quote> CPUs that are not running RT
          applications.</para>
      </listitem>
      <listitem>
        <para>Some kernel threads cannot be controlled with cpuset</para>
        <para>Performing heavy I/O in the VM may cause kthreads to be
          scheduled on the CPUs dedicated for RT. This can occur, for
          example, when a kthread is flushing dirty pages to disk.
        </para>
        <para>While it is impossible to move some kworker threads into
          the <systemitem>system</systemitem> cpuset, the above issue can be
          mitigated by setting the CPU affinity for those threads via:</para>
        <screen>/sys/devices/virtual/workqueue/writeback/cpumask</screen>
      </listitem>
    </orderedlist>
  </sect1>
  
  <sect1 xml:id="sec.virtguide.rec">
    <title>Recommendations</title>
    <para>Suggestions for tuning machines running both RT and KVM
      workloads are as follows:</para>
    <orderedlist>
      <listitem>
        <para>Affinitize RT tasks to their own CPUs, and if possible, to
          CPUs on their own dedicated socket. Using a dedicated socket
          avoids the issue from observation <xref
            linkend="li.virtguide.observ.vmheavyio"/> above where the LLC
          occupancy is churned by VMs doing lots of I/O operations. If
          that is not an option some customers may want to look at
          Intel's Cache Allocation Technology to further enforce cache
          allocation policies.</para>
      </listitem>
      <listitem>
        <para>Disable drivers that arm per-cpu timers such as cpufreq
          drivers, for example, <code>intel_pstate=disable</code>.</para>
      </listitem>
      <listitem>
        <para>Set IRQ affinity to CPUs that are not running RT workloads
          and disable irqbalance.</para>
      </listitem>
      <listitem>
        <para>Set IRQ affinity to CPUs that are not running RT workloads.
          This can be achieved by setting the <envar>IRQBALANCE_BANNED_CPUS</envar>
          environment variable used by <command>irqbalance</command>(1) with
          a bitmask of banned CPUs. For the examples used throughout this
          document the following setting was used:</para>
        <screen>IRQBALANCE_BANNED_CPUS="ffff,ffffff00"</screen>
      </listitem>
      <listitem>
        <para>Search for cpumask control files in 
          <filename>/sys</filename> and set them
          appropriately for those cases that cannot be controlled via
          cpuset. The following command will list those files:</para>
        <screen>find /sys -name cpumask</screen>
      </listitem>
    </orderedlist>
  </sect1>
</article>
